  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]


 17%|███████████████▌                                                                             | 10/60 [00:05<00:24,  2.02it/s]


 30%|███████████████████████████▉                                                                 | 18/60 [00:09<00:21,  1.99it/s]



 52%|████████████████████████████████████████████████                                             | 31/60 [00:15<00:14,  2.01it/s]


 65%|████████████████████████████████████████████████████████████▍                                | 39/60 [00:19<00:11,  1.90it/s]



 85%|███████████████████████████████████████████████████████████████████████████████              | 51/60 [00:25<00:04,  2.06it/s]


 98%|███████████████████████████████████████████████████████████████████████████████████████████▍ | 59/60 [00:29<00:00,  1.98it/s]
{'loss': 3.3679, 'grad_norm': 31.680465698242188, 'learning_rate': 9.259259259259259e-07, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:29<00:00,  2.00it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a detailed photo of a lunch box filled with meat, vegetables, and broccoles on a blue table
INFO:__main__:Generated caption: a detailed image showing a lunch box of food on a blue table with a yellow container of meat, broccote and vegetables in it
INFO:__main__:Model saved to models/vlm/gen_1
INFO:__main__:Training generation 2
INFO:__main__:Generating captions for generation 1
INFO:__main__:Loading BLIP model from models/vlm/gen_1
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Generation 1 parameters: temperature=0.95, top_p=0.92, top_k=40, repetition_penalty=1.30, num_beams=4
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.














Generating captions: 100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]
INFO:__main__:Generated 100 captions
INFO:__main__:Using checkpoint from models/vlm/gen_1
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_2
INFO:__main__:Using all 100 images for training in gen_2
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 2 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]


 15%|██████████████                                                                                | 9/60 [00:04<00:24,  2.12it/s]


 30%|███████████████████████████▉                                                                 | 18/60 [00:08<00:20,  2.01it/s]



 50%|██████████████████████████████████████████████▌                                              | 30/60 [00:14<00:13,  2.15it/s]


 65%|████████████████████████████████████████████████████████████▍                                | 39/60 [00:18<00:10,  2.00it/s]


 78%|████████████████████████████████████████████████████████████████████████▊                    | 47/60 [00:22<00:06,  1.94it/s]


100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28<00:00,  2.08it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
{'loss': 1.0654, 'grad_norm': 26.71706199645996, 'learning_rate': 0.0, 'epoch': 8.64}
{'train_runtime': 28.8037, 'train_samples_per_second': 34.718, 'train_steps_per_second': 2.083, 'train_loss': 1.907854684193929, 'epoch': 8.64}
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a detailed image showing a lunch box of food on a blue table with a pink container filled with meat, broccote and vegetables
INFO:__main__:Generated caption: a high resolution photo that shows a lunch box of food on a blue table with a pink container filled with meat, broccote and vegetables
INFO:__main__:Model saved to models/vlm/gen_2
INFO:__main__:Training generation 3
INFO:__main__:Generating captions for generation 2
INFO:__main__:Loading BLIP model from models/vlm/gen_2
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Generation 2 parameters: temperature=1.10, top_p=0.94, top_k=30, repetition_penalty=1.40, num_beams=3
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   9%|██████▍                                                                 | 9/100 [00:02<00:24,  3.73it/s]
Generating captions:  17%|████████████                                                           | 17/100 [00:04<00:21,  3.84it/s]
Generating captions:  23%|████████████████▎                                                      | 23/100 [00:06<00:25,  2.98it/s]
Generating captions:  30%|█████████████████████▎                                                 | 30/100 [00:08<00:21,  3.28it/s]
Generating captions:  37%|██████████████████████████▎                                            | 37/100 [00:10<00:19,  3.29it/s]
Generating captions:  44%|███████████████████████████████▏                                       | 44/100 [00:12<00:17,  3.17it/s]
Generating captions:  51%|████████████████████████████████████▏                                  | 51/100 [00:14<00:13,  3.54it/s]
Generating captions:  59%|█████████████████████████████████████████▉                             | 59/100 [00:16<00:10,  4.04it/s]
Generating captions:  67%|███████████████████████████████████████████████▌                       | 67/100 [00:18<00:08,  3.95it/s]
Generating captions:  73%|███████████████████████████████████████████████████▊                   | 73/100 [00:20<00:07,  3.51it/s]
Generating captions:  81%|█████████████████████████████████████████████████████████▌             | 81/100 [00:22<00:05,  3.49it/s]
Generating captions:  88%|██████████████████████████████████████████████████████████████▍        | 88/100 [00:24<00:03,  3.39it/s]
Generating captions:  96%|████████████████████████████████████████████████████████████████████▏  | 96/100 [00:26<00:01,  3.87it/s]
INFO:__main__:Found 100 images in data/vlm_training/gen_0███████████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
{'loss': 1.0976, 'grad_norm': 22.723997116088867, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
{'loss': 0.4757, 'grad_norm': 13.61269760131836, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.96}
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
{'loss': 0.196, 'grad_norm': 6.933647155761719, 'learning_rate': 2.777777777777778e-05, 'epoch': 4.32}
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
{'loss': 0.1085, 'grad_norm': 4.072019100189209, 'learning_rate': 1.8518518518518518e-05, 'epoch': 5.8}
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
{'loss': 0.0596, 'grad_norm': 2.7903928756713867, 'learning_rate': 9.259259259259259e-06, 'epoch': 7.16}
INFO:__main__:Starting training for generation 3 with 10 epochs█████████████████████████████████| 100/100 [00:27<00:00,  3.60it/s]
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...ase use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...ase use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0497, 'grad_norm': 2.3860623836517334, 'learning_rate': 0.0, 'epoch': 8.64}
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.3455, 'grad_norm': 2.2120916843414307, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0408, 'grad_norm': 0.728320300579071, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.96}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0188, 'grad_norm': 0.3503912091255188, 'learning_rate': 2.777777777777778e-05, 'epoch': 4.32}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0095, 'grad_norm': 0.19257278740406036, 'learning_rate': 1.8518518518518518e-05, 'epoch': 5.8}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0047, 'grad_norm': 0.13635969161987305, 'learning_rate': 9.259259259259259e-06, 'epoch': 7.16}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0037, 'grad_norm': 0.12377993762493134, 'learning_rate': 0.0, 'epoch': 8.64}
Generating captions:   3%|██▏                                                                     | 3/100 [00:00<00:27,  3.50it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  12%|████████▌                                                              | 12/100 [00:02<00:20,  4.37it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  21%|██████████████▉                                                        | 21/100 [00:04<00:17,  4.47it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  31%|██████████████████████                                                 | 31/100 [00:06<00:15,  4.47it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  40%|████████████████████████████▍                                          | 40/100 [00:08<00:13,  4.56it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  50%|███████████████████████████████████▌                                   | 50/100 [00:10<00:10,  4.66it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  59%|█████████████████████████████████████████▉                             | 59/100 [00:12<00:08,  4.67it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  68%|████████████████████████████████████████████████▎                      | 68/100 [00:14<00:06,  4.74it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  68%|████████████████████████████████████████████████▎                      | 68/100 [00:14<00:06,  4.74it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  68%|████████████████████████████████████████████████▎                      | 68/100 [00:14<00:06,  4.74it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  68%|████████████████████████████████████████████████▎                      | 68/100 [00:14<00:06,  4.74it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.6211, 'grad_norm': 1.8338418006896973, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.1372, 'grad_norm': 0.9317952990531921, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.96}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0462, 'grad_norm': 0.4377107322216034, 'learning_rate': 2.777777777777778e-05, 'epoch': 4.32}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0195, 'grad_norm': 0.3927432894706726, 'learning_rate': 1.8518518518518518e-05, 'epoch': 5.8}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0075, 'grad_norm': 0.11702451854944229, 'learning_rate': 9.259259259259259e-06, 'epoch': 7.16}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0054, 'grad_norm': 0.10130274295806885, 'learning_rate': 0.0, 'epoch': 8.64}
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.4966, 'grad_norm': 1.7444006204605103, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.1216, 'grad_norm': 0.6727570295333862, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.96}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0384, 'grad_norm': 0.47746577858924866, 'learning_rate': 2.777777777777778e-05, 'epoch': 4.32}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0147, 'grad_norm': 0.24784120917320251, 'learning_rate': 1.8518518518518518e-05, 'epoch': 5.8}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0065, 'grad_norm': 0.08809082955121994, 'learning_rate': 9.259259259259259e-06, 'epoch': 7.16}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.ulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0051, 'grad_norm': 0.08410783857107162, 'learning_rate': 0.0, 'epoch': 8.64}
INFO:__main__:Training generation 7th gradient checkpointing. Setting `use_cache=False`...ase use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   0%|                                                                                | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.3575, 'grad_norm': 1.2664114236831665, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0737, 'grad_norm': 2.097259044647217, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.96}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0239, 'grad_norm': 0.47002848982810974, 'learning_rate': 2.777777777777778e-05, 'epoch': 4.32}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0113, 'grad_norm': 0.25211331248283386, 'learning_rate': 1.8518518518518518e-05, 'epoch': 5.8}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0042, 'grad_norm': 0.07580342143774033, 'learning_rate': 9.259259259259259e-06, 'epoch': 7.16}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0037, 'grad_norm': 0.07334998995065689, 'learning_rate': 0.0, 'epoch': 8.64}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:   6%|████▎                                                                   | 6/100 [00:01<00:22,  4.19it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  15%|██████████▋                                                            | 15/100 [00:03<00:17,  4.72it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  23%|████████████████▎                                                      | 23/100 [00:05<00:17,  4.44it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  32%|██████████████████████▋                                                | 32/100 [00:07<00:14,  4.62it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  40%|████████████████████████████▍                                          | 40/100 [00:09<00:14,  4.19it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  49%|██████████████████████████████████▊                                    | 49/100 [00:11<00:11,  4.30it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  58%|█████████████████████████████████████████▏                             | 58/100 [00:13<00:09,  4.20it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  66%|██████████████████████████████████████████████▊                        | 66/100 [00:15<00:08,  3.99it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  66%|██████████████████████████████████████████████▊                        | 66/100 [00:15<00:08,  3.99it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  66%|██████████████████████████████████████████████▊                        | 66/100 [00:15<00:08,  3.99it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  66%|██████████████████████████████████████████████▊                        | 66/100 [00:15<00:08,  3.99it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0853, 'grad_norm': 0.7798532247543335, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0284, 'grad_norm': 0.6436251401901245, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.96}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.01, 'grad_norm': 0.1909126341342926, 'learning_rate': 2.777777777777778e-05, 'epoch': 4.32}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0042, 'grad_norm': 0.05784593150019646, 'learning_rate': 1.8518518518518518e-05, 'epoch': 5.8}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0021, 'grad_norm': 0.041443075984716415, 'learning_rate': 9.259259259259259e-06, 'epoch': 7.16}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0018, 'grad_norm': 0.03132706508040428, 'learning_rate': 0.0, 'epoch': 8.64}
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\generation\configuration_utils.py:695: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `1.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.034, 'grad_norm': 0.7921033501625061, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0196, 'grad_norm': 0.6556830406188965, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.96}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0065, 'grad_norm': 0.37421950697898865, 'learning_rate': 2.777777777777778e-05, 'epoch': 4.32}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0021, 'grad_norm': 0.08301801979541779, 'learning_rate': 1.8518518518518518e-05, 'epoch': 5.8}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0012, 'grad_norm': 0.03495045378804207, 'learning_rate': 9.259259259259259e-06, 'epoch': 7.16}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).-based generation modes. You should set `num_beams>1` or unset `length_penalty`.s strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0009, 'grad_norm': 0.018507104367017746, 'learning_rate': 0.0, 'epoch': 8.64}
Generating captions:   3%|██▏                                                                     | 3/100 [00:00<00:25,  3.76it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  12%|████████▌                                                              | 12/100 [00:02<00:20,  4.22it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  20%|██████████████▏                                                        | 20/100 [00:04<00:20,  3.95it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  28%|███████████████████▉                                                   | 28/100 [00:06<00:17,  4.02it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  36%|█████████████████████████▌                                             | 36/100 [00:08<00:15,  4.10it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  44%|███████████████████████████████▏                                       | 44/100 [00:10<00:14,  3.85it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  52%|████████████████████████████████████▉                                  | 52/100 [00:12<00:12,  3.90it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  60%|██████████████████████████████████████████▌                            | 60/100 [00:14<00:09,  4.21it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  60%|██████████████████████████████████████████▌                            | 60/100 [00:14<00:09,  4.21it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  60%|██████████████████████████████████████████▌                            | 60/100 [00:14<00:09,  4.21it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  60%|██████████████████████████████████████████▌                            | 60/100 [00:14<00:09,  4.21it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Generating captions:  60%|██████████████████████████████████████████▌                            | 60/100 [00:14<00:09,  4.21it/s]M. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0238, 'grad_norm': 0.24963067471981049, 'learning_rate': 4.62962962962963e-05, 'epoch': 1.48}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.011, 'grad_norm': 0.5905368328094482, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.96}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0039, 'grad_norm': 0.062058866024017334, 'learning_rate': 2.777777777777778e-05, 'epoch': 4.32}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0015, 'grad_norm': 0.02502257190644741, 'learning_rate': 1.8518518518518518e-05, 'epoch': 5.8}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0007, 'grad_norm': 0.012108980678021908, 'learning_rate': 9.259259259259259e-06, 'epoch': 7.16}
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
{'loss': 0.0006, 'grad_norm': 0.012017570436000824, 'learning_rate': 0.0, 'epoch': 8.64}
INFO:__main__:Generated caption: a detailed scene showing the plastic lunch box of food on an cloth with red and blue glass container filled in meat, broccote & isDefaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
INFO:__main__:Generated caption: a detailed scene showing the plastic lunch box of food on an cloth with red and blue glass container filled in meat, broccote & isDefaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.nces (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.