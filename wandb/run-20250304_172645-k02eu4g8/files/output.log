  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]


 17%|███████████████▌                                                                             | 10/60 [00:05<00:25,  2.00it/s]


 30%|███████████████████████████▉                                                                 | 18/60 [00:09<00:21,  1.93it/s]



 52%|████████████████████████████████████████████████                                             | 31/60 [00:15<00:14,  2.03it/s]


 65%|████████████████████████████████████████████████████████████▍                                | 39/60 [00:19<00:10,  1.98it/s]


 78%|████████████████████████████████████████████████████████████████████████▊                    | 47/60 [00:23<00:06,  1.88it/s]

 87%|████████████████████████████████████████████████████████████████████████████████▌            | 52/60 [00:25<00:04,  1.93it/s]Traceback (most recent call last):
  File "C:\Users\zizha\development\FusionSpaceModalityGap\train_vlm_captioning.py", line 1190, in <module>
    main()
  File "C:\Users\zizha\development\FusionSpaceModalityGap\train_vlm_captioning.py", line 1180, in main
    final_gen = run_generation_loop(
                ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zizha\development\FusionSpaceModalityGap\train_vlm_captioning.py", line 1152, in run_generation_loop
    output_model_path = train_vlm(
                        ^^^^^^^^^^
  File "C:\Users\zizha\development\FusionSpaceModalityGap\train_vlm_captioning.py", line 477, in train_vlm
    trainer.train()
  File "C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\transformers\trainer.py", line 3712, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\accelerate\accelerator.py", line 2242, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\autograd\__init__.py", line 289, in backward
    _engine_run_backward(
  File "C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\autograd\graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt