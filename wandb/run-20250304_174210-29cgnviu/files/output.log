  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]


 17%|███████████████▌                                                                             | 10/60 [00:05<00:26,  1.86it/s]


 28%|██████████████████████████▎                                                                  | 17/60 [00:09<00:22,  1.88it/s]



 48%|████████████████████████████████████████████▉                                                | 29/60 [00:15<00:14,  2.09it/s]



 67%|██████████████████████████████████████████████████████████████                               | 40/60 [00:21<00:11,  1.82it/s]


 82%|███████████████████████████████████████████████████████████████████████████▉                 | 49/60 [00:25<00:04,  2.33it/s]



100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:31<00:00,  1.80it/s]
{'loss': 3.3679, 'grad_norm': 31.680465698242188, 'learning_rate': 9.259259259259259e-07, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:31<00:00,  1.89it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a detailed photo of a lunch box filled with meat, vegetables, and broccoles on a blue table
INFO:__main__:Generated caption: a detailed image showing a lunch box of food on a blue table with a yellow container of meat, broccote, and vegetables in it is on top of the other vegetables
INFO:__main__:Model saved to models/vlm/gen_1
INFO:__main__:Training generation 2
INFO:__main__:Using checkpoint from models/vlm/gen_1
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_2
INFO:__main__:Using all 100 images for training in gen_2
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 2 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]


 17%|███████████████▌                                                                             | 10/60 [00:05<00:26,  1.91it/s]


 30%|███████████████████████████▉                                                                 | 18/60 [00:09<00:22,  1.87it/s]



 50%|██████████████████████████████████████████████▌                                              | 30/60 [00:15<00:15,  1.96it/s]


 62%|█████████████████████████████████████████████████████████▎                                   | 37/60 [00:19<00:11,  1.99it/s]



 82%|███████████████████████████████████████████████████████████████████████████▉                 | 49/60 [00:25<00:04,  2.36it/s]


 95%|████████████████████████████████████████████████████████████████████████████████████████▎    | 57/60 [00:29<00:01,  2.18it/s]
{'loss': 1.0654, 'grad_norm': 26.71706199645996, 'learning_rate': 0.0, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30<00:00,  1.95it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a detailed image showing a lunch box of food on a blue table with a pink container filled with meat, broccote and vegetables
INFO:__main__:Generated caption: a high resolution photo that shows a lunch box of food on a blue table with a pink container filled with meat, broccote, and vegetables
INFO:__main__:Model saved to models/vlm/gen_2
INFO:__main__:Training generation 3
INFO:__main__:Using checkpoint from models/vlm/gen_2
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_3
INFO:__main__:Using all 100 images for training in gen_3
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 3 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]

 13%|████████████▌                                                                                 | 8/60 [00:04<00:24,  2.13it/s]



 32%|█████████████████████████████▍                                                               | 19/60 [00:09<00:21,  1.87it/s]


 47%|███████████████████████████████████████████▍                                                 | 28/60 [00:14<00:13,  2.39it/s]



 65%|████████████████████████████████████████████████████████████▍                                | 39/60 [00:19<00:11,  1.90it/s]


 78%|████████████████████████████████████████████████████████████████████████▊                    | 47/60 [00:23<00:06,  1.87it/s]



 98%|███████████████████████████████████████████████████████████████████████████████████████████▍ | 59/60 [00:29<00:00,  1.94it/s]
{'loss': 0.0497, 'grad_norm': 2.3860623836517334, 'learning_rate': 0.0, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30<00:00,  1.98it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a high resolution photo that shows a lunch box of food on a blue table with a pink container filled with meat, broccote and vegetables
INFO:__main__:Generated caption: a clear, detailed picture of a lunch box with meat, broccote and vegetables in it on a blue table with a pink container on top of the picture
INFO:__main__:Model saved to models/vlm/gen_3
INFO:__main__:Training generation 4
INFO:__main__:Using checkpoint from models/vlm/gen_3
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_4
INFO:__main__:Using all 100 images for training in gen_4
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 4 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]


 17%|███████████████▌                                                                             | 10/60 [00:05<00:25,  1.93it/s]


 30%|███████████████████████████▉                                                                 | 18/60 [00:09<00:22,  1.89it/s]



 50%|██████████████████████████████████████████████▌                                              | 30/60 [00:15<00:14,  2.06it/s]


 63%|██████████████████████████████████████████████████████████▉                                  | 38/60 [00:19<00:11,  1.95it/s]



 83%|█████████████████████████████████████████████████████████████████████████████▌               | 50/60 [00:25<00:04,  2.15it/s]


 97%|█████████████████████████████████████████████████████████████████████████████████████████▉   | 58/60 [00:29<00:00,  2.04it/s]
{'loss': 0.0037, 'grad_norm': 0.12377993762493134, 'learning_rate': 0.0, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30<00:00,  1.98it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a clear, detailed picture of a lunch box of food on a blue table with a pink container filled with meat, broccote and vegetables
INFO:__main__:Generated caption: a photo with many details showing a lunch box of food on a blue table with a pink container filled with meat, broccote and broccte
INFO:__main__:Model saved to models/vlm/gen_4
INFO:__main__:Training generation 5
INFO:__main__:Using checkpoint from models/vlm/gen_4
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_5
INFO:__main__:Using all 100 images for training in gen_5
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 5 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]


 15%|██████████████                                                                                | 9/60 [00:04<00:25,  2.02it/s]


 28%|██████████████████████████▎                                                                  | 17/60 [00:08<00:21,  1.97it/s]



 48%|████████████████████████████████████████████▉                                                | 29/60 [00:14<00:14,  2.16it/s]


 62%|█████████████████████████████████████████████████████████▎                                   | 37/60 [00:18<00:11,  2.03it/s]



 82%|███████████████████████████████████████████████████████████████████████████▉                 | 49/60 [00:24<00:04,  2.40it/s]


 95%|████████████████████████████████████████████████████████████████████████████████████████▎    | 57/60 [00:28<00:01,  2.17it/s]
{'loss': 0.0054, 'grad_norm': 0.10130274295806885, 'learning_rate': 0.0, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30<00:00,  1.98it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a photo with many details showing a plastic lunch box of food on a blue table cloth with a pink container filled in meat, broccote and a little piece of vegetables & brocc
INFO:__main__:Generated caption: a comprehensive image depicting a lunch box of food on a blue table cloth with a pink container filled in meat, broccote and a little piece of vegetables & brocc
INFO:__main__:Model saved to models/vlm/gen_5
INFO:__main__:Training generation 6
INFO:__main__:Using checkpoint from models/vlm/gen_5
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_6
INFO:__main__:Using all 100 images for training in gen_6
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 6 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]

 13%|████████████▌                                                                                 | 8/60 [00:04<00:24,  2.14it/s]



 32%|█████████████████████████████▍                                                               | 19/60 [00:09<00:21,  1.87it/s]


 47%|███████████████████████████████████████████▍                                                 | 28/60 [00:14<00:13,  2.39it/s]



 67%|██████████████████████████████████████████████████████████████                               | 40/60 [00:20<00:10,  1.88it/s]


 80%|██████████████████████████████████████████████████████████████████████████▍                  | 48/60 [00:24<00:06,  1.86it/s]


100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30<00:00,  1.99it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
{'loss': 0.0051, 'grad_norm': 0.08410783857107162, 'learning_rate': 0.0, 'epoch': 8.64}
{'train_runtime': 30.1214, 'train_samples_per_second': 33.199, 'train_steps_per_second': 1.992, 'train_loss': 0.11381563072403272, 'epoch': 8.64}
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a comprehensive image depicting the lunch box of food on an table cloth with a pink plastic container filled in meat, broccote and
INFO:__main__:Generated caption: a detailed scene showing the lunch box of food on an table cloth with a pink plastic container filled in meat, broccote and vecc garden
INFO:__main__:Model saved to models/vlm/gen_6
INFO:__main__:Training generation 7
INFO:__main__:Using checkpoint from models/vlm/gen_6
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_7
INFO:__main__:Using all 100 images for training in gen_7
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 7 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]

 12%|██████████▉                                                                                   | 7/60 [00:03<00:22,  2.40it/s]



 32%|█████████████████████████████▍                                                               | 19/60 [00:09<00:22,  1.85it/s]


 45%|█████████████████████████████████████████▊                                                   | 27/60 [00:13<00:17,  1.87it/s]



 65%|████████████████████████████████████████████████████████████▍                                | 39/60 [00:19<00:11,  1.89it/s]


 78%|████████████████████████████████████████████████████████████████████████▊                    | 47/60 [00:23<00:06,  1.88it/s]



 98%|███████████████████████████████████████████████████████████████████████████████████████████▍ | 59/60 [00:29<00:00,  1.96it/s]
{'loss': 0.0037, 'grad_norm': 0.07334998995065689, 'learning_rate': 0.0, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30<00:00,  1.98it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a detailed scene showing the plastic lunch box of food on an table cloth with red and blue glass container filled in meat, broccote &
INFO:__main__:Generated caption: a detailed scene showing the plastic lunch box of food on an table cloth with red and blue glass container filled in meat, broccote & other
INFO:__main__:Model saved to models/vlm/gen_7
INFO:__main__:Training generation 8
INFO:__main__:Using checkpoint from models/vlm/gen_7
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_8
INFO:__main__:Using all 100 images for training in gen_8
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 8 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]


 17%|███████████████▌                                                                             | 10/60 [00:05<00:26,  1.91it/s]


 28%|██████████████████████████▎                                                                  | 17/60 [00:08<00:22,  1.92it/s]



 50%|██████████████████████████████████████████████▌                                              | 30/60 [00:15<00:14,  2.03it/s]


 62%|█████████████████████████████████████████████████████████▎                                   | 37/60 [00:18<00:11,  2.00it/s]



 83%|█████████████████████████████████████████████████████████████████████████████▌               | 50/60 [00:25<00:04,  2.16it/s]


 97%|█████████████████████████████████████████████████████████████████████████████████████████▉   | 58/60 [00:29<00:00,  2.07it/s]
{'loss': 0.0018, 'grad_norm': 0.03132706508040428, 'learning_rate': 0.0, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30<00:00,  1.97it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a detailed scene showing the plastic lunch box of food on an cloth with red and blue glass container filled in meat, broccote & is
INFO:__main__:Generated caption: a detailed scene showing the plastic lunch box of food on an cloth with red and blue glass container filled in meat, broccote & is image
INFO:__main__:Model saved to models/vlm/gen_8
INFO:__main__:Training generation 9
INFO:__main__:Using checkpoint from models/vlm/gen_8
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_9
INFO:__main__:Using all 100 images for training in gen_9
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 9 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]


 15%|██████████████                                                                                | 9/60 [00:04<00:25,  2.02it/s]



 35%|████████████████████████████████▌                                                            | 21/60 [00:10<00:16,  2.41it/s]


 48%|████████████████████████████████████████████▉                                                | 29/60 [00:14<00:14,  2.18it/s]



 67%|██████████████████████████████████████████████████████████████                               | 40/60 [00:20<00:10,  1.87it/s]


 82%|███████████████████████████████████████████████████████████████████████████▉                 | 49/60 [00:24<00:04,  2.46it/s]


 95%|████████████████████████████████████████████████████████████████████████████████████████▎    | 57/60 [00:28<00:01,  2.16it/s]
{'loss': 0.0009, 'grad_norm': 0.018507104367017746, 'learning_rate': 0.0, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:30<00:00,  2.00it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a detailed scene showing the plastic lunch box of food on an cloth with red and blue glass container filled in meat, broccote & is
INFO:__main__:Generated caption: a detailed scene showing the plastic lunch box of food on an cloth with red and blue glass container filled in meat, broccote & is details
INFO:__main__:Model saved to models/vlm/gen_9
INFO:__main__:Training generation 10
INFO:__main__:Using checkpoint from models/vlm/gen_9
INFO:__main__:Found 100 images for training
INFO:__main__:Loaded 414113 captions from data/coco/annotations/captions_train2014.json
INFO:__main__:Found 100 images in data/vlm_training/gen_0
INFO:__main__:Created dataset with 100 image-caption pairs for gen_10
INFO:__main__:Using all 100 images for training in gen_10
INFO:__main__:Loading model with aggressive memory optimizations
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:__main__:Starting training for generation 10 with 10 epochs
  0%|                                                                                                      | 0/60 [00:00<?, ?it/s]C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]

 13%|████████████▌                                                                                 | 8/60 [00:04<00:24,  2.15it/s]



 35%|████████████████████████████████▌                                                            | 21/60 [00:10<00:15,  2.52it/s]


 48%|████████████████████████████████████████████▉                                                | 29/60 [00:14<00:13,  2.26it/s]


 62%|█████████████████████████████████████████████████████████▎                                   | 37/60 [00:17<00:10,  2.14it/s]



 83%|█████████████████████████████████████████████████████████████████████████████▌               | 50/60 [00:23<00:04,  2.27it/s]


 98%|███████████████████████████████████████████████████████████████████████████████████████████▍ | 59/60 [00:28<00:00,  2.05it/s]
{'loss': 0.0006, 'grad_norm': 0.012017570436000824, 'learning_rate': 0.0, 'epoch': 8.64}
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:28<00:00,  2.08it/s]
INFO:__main__:Training completed successfully
INFO:__main__:Saving model
INFO:__main__:Generating caption for the first training image to evaluate the model
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
C:\Users\zizha\anaconda3\envs\311\Lib\site-packages\torch\utils\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
INFO:__main__:Original caption: a detailed scene showing the plastic lunch box of food on an cloth with red and blue glass container filled in meat, broccote & is
INFO:__main__:Generated caption: a detailed scene showing the plastic lunch box of food on an cloth with red and blue glass container filled in meat, broccote & is sitting down in
INFO:__main__:Model saved to models/vlm/gen_10
INFO:__main__:Training completed through generation 10
INFO:__main__:Final model saved at: models/vlm/gen_10