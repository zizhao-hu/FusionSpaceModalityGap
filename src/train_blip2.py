import os
import json
import torch
import numpy as np
from PIL import Image
import random
from tqdm import tqdm
from transformers import (
    AutoProcessor, 
    AutoModelForCausalLM,
    Blip2ForConditionalGeneration,
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import logging
from torch.utils.data import DataLoader
import evaluate
import argparse
from datetime import datetime
import shutil
import glob

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_coco_captions(annotation_file):
    """Load COCO captions and return a dictionary mapping image IDs to captions."""
    with open(annotation_file, 'r') as f:
        annotations = json.load(f)
    
    logger.info(f"Loaded {len(annotations['annotations'])} captions from {annotation_file}")
    
    # Create a mapping from image ID to captions
    image_id_to_captions = {}
    for ann in annotations['annotations']:
        image_id = str(ann['image_id'])
        if image_id not in image_id_to_captions:
            image_id_to_captions[image_id] = []
        image_id_to_captions[image_id].append(ann['caption'])
    
    return image_id_to_captions

def create_dataset(data_dir, annotation_file, processor, max_samples=None, gen_number=0, training_mode="recursive"):
    """Create a dataset for training the VLM.
    
    Args:
        data_dir: Directory containing training images
        annotation_file: Path to COCO annotations file
        processor: The model processor
        max_samples: Maximum number of samples to use
        gen_number: Current generation number
        training_mode: One of ["recursive", "baseline", "gen0"]
            - recursive: Use captions from previous generation (default)
            - baseline: Use original COCO captions
            - gen0: Use captions generated by gen_0 model
    """
    # Load COCO captions
    coco_captions = load_coco_captions(annotation_file)
    
    # Use images from the train/real directory
    train_dir = "data/vlm/train/images/real"
    image_paths = glob.glob(os.path.join(train_dir, "*.jpg"))
    if not image_paths:
        image_paths = glob.glob(os.path.join(train_dir, "*.png"))
    
    logger.info(f"Found {len(image_paths)} images in {train_dir}")
    
    dataset = []
    
    if training_mode == "baseline":
        # Use original COCO captions for all generations
        captions_file = "data/vlm/train/captions/real/captions.json"
        with open(captions_file, "r") as f:
            real_captions = json.load(f)
        
        # Create a mapping from image path to caption
        image_to_caption = {item["image_path"]: item["caption"] for item in real_captions}
        
        for image_path in image_paths:
            # Normalize path to handle different path separators
            norm_path = os.path.normpath(image_path)
            
            # Try different path formats to match with captions
            if norm_path in image_to_caption:
                caption = image_to_caption[norm_path]
            else:
                # Try with forward slashes
                alt_path = norm_path.replace('\\', '/')
                if alt_path in image_to_caption:
                    caption = image_to_caption[alt_path]
                else:
                    # Try with just the filename
                    filename = os.path.basename(image_path)
                    found = False
                    for path, cap in image_to_caption.items():
                        if filename in path:
                            caption = cap
                            found = True
                            break
                    
                    if not found:
                        logger.warning(f"No caption found for {image_path}, skipping")
                        continue
            
            dataset.append({
                "image_path": image_path,
                "caption": caption
            })
    
    elif training_mode == "gen0":
        # Use captions generated by gen_0 model
        gen0_captions_file = "data/vlm/train/captions/gen_0/captions.json"
        if not os.path.exists(gen0_captions_file):
            raise FileNotFoundError(f"Gen_0 captions file {gen0_captions_file} not found")
        
        with open(gen0_captions_file, "r") as f:
            generated_captions = json.load(f)
        
        # Create a mapping from image path to generated caption
        image_to_caption = {item["image_path"]: item["caption"] for item in generated_captions}
        
        for image_path in image_paths:
            # Normalize path to handle different path separators
            norm_path = os.path.normpath(image_path)
            
            # Try different path formats to match with captions
            if norm_path in image_to_caption:
                caption = image_to_caption[norm_path]
            else:
                # Try with forward slashes
                alt_path = norm_path.replace('\\', '/')
                if alt_path in image_to_caption:
                    caption = image_to_caption[alt_path]
                else:
                    # Try with just the filename
                    filename = os.path.basename(image_path)
                    found = False
                    for path, cap in image_to_caption.items():
                        if filename in path:
                            caption = cap
                            found = True
                            break
                    
                    if not found:
                        logger.warning(f"No gen_0 caption found for {image_path}, skipping")
                        continue
            
            dataset.append({
                "image_path": image_path,
                "caption": caption
            })
    
    else:  # training_mode == "recursive"
        # For generation 0, use real captions
        # For generation 1+, use generated captions from previous generation
        if gen_number == 0:
            # Use real captions for gen_0
            captions_file = "data/vlm/train/captions/real/captions.json"
            with open(captions_file, "r") as f:
                real_captions = json.load(f)
            
            # Create a mapping from image path to caption
            image_to_caption = {item["image_path"]: item["caption"] for item in real_captions}
            
            for image_path in image_paths:
                # Normalize path to handle different path separators
                norm_path = os.path.normpath(image_path)
                
                # Try different path formats to match with captions
                if norm_path in image_to_caption:
                    caption = image_to_caption[norm_path]
                else:
                    # Try with forward slashes
                    alt_path = norm_path.replace('\\', '/')
                    if alt_path in image_to_caption:
                        caption = image_to_caption[alt_path]
                    else:
                        # Try with just the filename
                        filename = os.path.basename(image_path)
                        found = False
                        for path, cap in image_to_caption.items():
                            if filename in path:
                                caption = cap
                                found = True
                                break
                        
                        if not found:
                            logger.warning(f"No caption found for {image_path}, skipping")
                            continue
                
                dataset.append({
                    "image_path": image_path,
                    "caption": caption
                })
        else:
            # Use generated captions from previous generation
            prev_gen = gen_number - 1
            captions_file = f"data/vlm/train/captions/gen_{prev_gen}/captions.json"
            
            if not os.path.exists(captions_file):
                raise FileNotFoundError(f"Generated captions file {captions_file} not found")
            
            with open(captions_file, "r") as f:
                generated_captions = json.load(f)
            
            # Create a mapping from image path to generated caption
            image_to_caption = {item["image_path"]: item["caption"] for item in generated_captions}
            
            for image_path in image_paths:
                # Normalize path to handle different path separators
                norm_path = os.path.normpath(image_path)
                
                # Try different path formats to match with captions
                if norm_path in image_to_caption:
                    caption = image_to_caption[norm_path]
                else:
                    # Try with forward slashes
                    alt_path = norm_path.replace('\\', '/')
                    if alt_path in image_to_caption:
                        caption = image_to_caption[alt_path]
                    else:
                        # Try with just the filename
                        filename = os.path.basename(image_path)
                        found = False
                        for path, cap in image_to_caption.items():
                            if filename in path:
                                caption = cap
                                found = True
                                break
                        
                        if not found:
                            logger.warning(f"No generated caption found for {image_path}, skipping")
                            continue
                
                dataset.append({
                    "image_path": image_path,
                    "caption": caption
                })
    
    # Limit dataset size if specified
    if max_samples and max_samples < len(dataset):
        dataset = dataset[:max_samples]
    
    logger.info(f"Created dataset with {len(dataset)} image-caption pairs for gen_{gen_number} using {training_mode} mode")
    
    return dataset

def check_training_data(data_dir, required_count=100):
    """Check if we have enough training data."""
    if not os.path.exists(data_dir):
        logger.info(f"Training data directory {data_dir} does not exist")
        return False
    
    image_files = [f for f in os.listdir(data_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
    if len(image_files) < required_count:
        logger.info(f"Found only {len(image_files)} images, need at least {required_count}")
        return False
    
    logger.info(f"Found {len(image_files)} images for training")
    return True

def generate_missing_images(data_dir, required_count=100):
    """Generate missing images if needed."""
    # Create the directory if it doesn't exist
    os.makedirs(data_dir, exist_ok=True)
    
    # Source COCO images from the training set
    coco_dir = "data/coco/train2014"
    if not os.path.exists(coco_dir):
        logger.error(f"COCO directory {coco_dir} does not exist. Please download COCO dataset first.")
        raise FileNotFoundError(f"COCO directory {coco_dir} not found")
    
    # Get list of COCO images
    coco_images = [f for f in os.listdir(coco_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
    
    if len(coco_images) == 0:
        logger.error(f"No images found in COCO directory {coco_dir}")
        raise FileNotFoundError(f"No images found in COCO directory {coco_dir}")
    
    # Sort images to ensure we get the first 100 consistently
    coco_images.sort()
    
    # For gen_0, use the first 100 images
    if "gen_0" in data_dir:
        logger.info("Using the first 100 images from COCO training set for gen_0")
        selected_images = coco_images[:required_count]
    else:
        # For other generations, select random images
        if len(coco_images) > required_count:
            selected_images = random.sample(coco_images, required_count)
        else:
            selected_images = coco_images
            logger.warning(f"Only found {len(coco_images)} images in COCO directory, using all of them")
    
    # Copy images to the target directory
    for img_file in tqdm(selected_images, desc=f"Copying images to {data_dir}"):
        src_path = os.path.join(coco_dir, img_file)
        dst_path = os.path.join(data_dir, img_file)
        try:
            # Use shutil.copy2 to preserve metadata
            shutil.copy2(src_path, dst_path)
        except Exception as e:
            logger.error(f"Error copying {src_path} to {dst_path}: {e}")
    
    logger.info(f"Copied {len(selected_images)} images to {data_dir}")

class VLMCaptioningDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, processor):
        self.dataset = dataset
        self.processor = processor
    
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        item = self.dataset[idx]
        
        # Load image
        image = Image.open(item["image_path"]).convert("RGB")
        
        # Process inputs
        encoding = self.processor(
            images=image,
            text=item["caption"],
            padding="max_length",
            max_length=77,
            truncation=True,
            return_tensors="pt"
        )
        
        # Remove batch dimension from text inputs but keep it for pixel_values
        batch = {
            "pixel_values": encoding["pixel_values"],  # Shape: (1, 3, 224, 224)
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
        }
        
        # Set labels for training
        batch["labels"] = batch["input_ids"].clone()
        
        return batch

# Move CustomDataCollator outside of train_vlm function to avoid pickling issues
class CustomDataCollator:
    def __init__(self, processor):
        self.processor = processor
        
    def __call__(self, features):
        # Separate pixel_values (which already have batch dimension)
        pixel_values = [feature["pixel_values"] for feature in features]
        
        # Concatenate along batch dimension
        pixel_values_batch = torch.cat(pixel_values, dim=0)
        
        # Get max length for padding
        max_length = max(len(feature["input_ids"]) for feature in features)
        
        # Initialize padded tensors
        input_ids = torch.full(
            (len(features), max_length),
            self.processor.tokenizer.pad_token_id,
            dtype=torch.long
        )
        attention_mask = torch.zeros(
            (len(features), max_length),
            dtype=torch.long
        )
        labels = torch.full(
            (len(features), max_length),
            -100,  # Ignore index for loss
            dtype=torch.long
        )
        
        # Fill tensors with actual values
        for i, feature in enumerate(features):
            seq_length = len(feature["input_ids"])
            input_ids[i, :seq_length] = feature["input_ids"]
            attention_mask[i, :seq_length] = feature["attention_mask"]
            labels[i, :seq_length] = feature["labels"]
        
        return {
            "pixel_values": pixel_values_batch,
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

def train_vlm(gen_number, input_model_path=None, epochs=10, batch_size=4, learning_rate=5e-5, skip_training=False, training_mode="recursive"):
    """Fine-tune a Vision-Language Model for captioning."""
    # Set up paths
    data_dir = "data/vlm_training/gen_0"  # Always use gen_0 images
    output_dir = f"models/vlm/gen_{gen_number}_{training_mode}"
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Determine which model to use
    if input_model_path:
        model_name = input_model_path
        logger.info(f"Using checkpoint from {model_name}")
    else:
        # Use a smaller pre-trained model to save memory
        model_name = "Salesforce/blip-image-captioning-base"  # Much smaller than blip2-opt-2.7b
        logger.info(f"Using smaller pre-trained model {model_name}")
    
    # Load processor
    processor = AutoProcessor.from_pretrained(model_name)
    
    # Check if we have training data (gen_0 images)
    if not check_training_data(data_dir):
        generate_missing_images(data_dir)
    
    # Create dataset with appropriate captions for this generation
    annotation_file = "data/coco/annotations/captions_train2014.json"
    dataset = create_dataset(data_dir, annotation_file, processor, gen_number=gen_number, training_mode=training_mode)
    
    # Create training dataset
    train_dataset = VLMCaptioningDataset(dataset, processor)
    
    # Use all images for training instead of just 1
    logger.info(f"Using all {len(train_dataset)} images for training in gen_{gen_number} with {training_mode} mode")
    
    # Option to skip training and just do inference
    if skip_training:
        logger.info("Skipping training as requested, loading model for inference only")
        # Load model in eval mode to save memory
        # Use the correct model class based on the model name
        if "blip-image-captioning" in model_name:
            from transformers import BlipForConditionalGeneration
            model = BlipForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="auto"
            )
        elif "blip2" in model_name:
            model = Blip2ForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="auto"
            )
        else:
            # For other models, try to determine the right class
            from transformers import AutoModelForVision2Seq
            model = AutoModelForVision2Seq.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="auto"
            )
        
        model.eval()  # Set to evaluation mode
        
        # Save the model directly
        logger.info("Saving model without training")
        model.save_pretrained(output_dir)
        processor.save_pretrained(output_dir)
    else:
        # Load model with aggressive memory optimizations
        logger.info("Loading model with aggressive memory optimizations")
        
        # Set PyTorch to aggressively empty cache
        torch.cuda.empty_cache()
        
        # Set environment variable to avoid memory fragmentation
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        
        # Load the correct model class based on the model name
        if "blip-image-captioning" in model_name:
            from transformers import BlipForConditionalGeneration
            model = BlipForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="auto",
                low_cpu_mem_usage=True,
                offload_folder="offload"
            )
        elif "blip2" in model_name:
            model = Blip2ForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map="auto",
                low_cpu_mem_usage=True,
                offload_folder="offload",
                use_cache=False  # Set use_cache to False to avoid warnings with gradient checkpointing
            )
        else:
            # For other models, try to determine the right class
            from transformers import AutoModelForVision2Seq
            # Check if the model is actually a BLIP model
            try:
                with open(os.path.join(model_name, "config.json"), "r") as f:
                    config = json.load(f)
                model_type = config.get("model_type", "").lower()
                
                if "blip" in model_type and "blip2" not in model_type:
                    # It's a BLIP model, load it without use_cache
                    from transformers import BlipForConditionalGeneration
                    model = BlipForConditionalGeneration.from_pretrained(
                        model_name,
                        torch_dtype=torch.float32,
                        device_map="auto",
                        low_cpu_mem_usage=True,
                        offload_folder="offload"
                    )
                else:
                    # It's not a BLIP model, load with use_cache=False
                    model = AutoModelForVision2Seq.from_pretrained(
                        model_name,
                        torch_dtype=torch.float32,
                        device_map="auto",
                        low_cpu_mem_usage=True,
                        offload_folder="offload",
                        use_cache=False  # Set use_cache to False to avoid warnings with gradient checkpointing
                    )
            except Exception as e:
                logger.warning(f"Error determining model type: {e}. Loading without use_cache parameter.")
                model = AutoModelForVision2Seq.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    offload_folder="offload"
                )
        
        # Create data collator
        data_collator = CustomDataCollator(processor)
        
        # Set up training arguments
        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=4,
            learning_rate=learning_rate,
            weight_decay=0.05,
            warmup_ratio=0.1,
            num_train_epochs=epochs,
            save_strategy="no",  # Don't save during training
            eval_strategy="no",
            logging_steps=10,
            remove_unused_columns=False,
            push_to_hub=False,
            disable_tqdm=False,
            fp16=True,
            gradient_checkpointing=True,
        )
        
        # Create trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=data_collator
        )
        
        # Free up memory before training
        import gc
        gc.collect()
        torch.cuda.empty_cache()
        
        try:
            # Train model for the full number of epochs
            logger.info(f"Starting training for generation {gen_number} with {epochs} epochs in {training_mode} mode")
            try:
                trainer.train()
                logger.info("Training completed successfully")
            except RuntimeError as e:
                if "FP16" in str(e) or "unscale" in str(e):
                    logger.warning(f"FP16 error during training: {e}. Trying with different settings.")
                    # Modify training args to avoid FP16 issues
                    trainer.args.fp16 = False
                    trainer.args.bf16 = False
                    trainer.args.optim = "adamw_torch"
                    # Try training again with modified settings
                    try:
                        trainer.train()
                        logger.info("Training completed successfully with modified settings")
                    except Exception as e2:
                        logger.error(f"Second training attempt also failed: {e2}")
                        logger.warning("Proceeding with model saving despite training errors")
                elif "CUDA out of memory" in str(e):
                    logger.warning("CUDA out of memory during training. Trying with smaller batch size.")
                    # Modify training args to use smaller batch size
                    trainer.args.per_device_train_batch_size = max(1, batch_size // 2)
                    trainer.args.gradient_accumulation_steps = 8  # Increase gradient accumulation
                    try:
                        trainer.train()
                        logger.info("Training completed successfully with smaller batch size")
                    except Exception as e2:
                        logger.error(f"Training with smaller batch size also failed: {e2}")
                        logger.warning("Falling back to inference-only mode.")
                        # Free memory and reload model in eval mode
                        del model, trainer
                        gc.collect()
                        torch.cuda.empty_cache()
                        
                        # Load model in eval mode with the correct class
                        if "blip-image-captioning" in model_name:
                            from transformers import BlipForConditionalGeneration
                            model = BlipForConditionalGeneration.from_pretrained(
                                model_name,
                                torch_dtype=torch.float32,
                                device_map="auto"
                            )
                        elif "blip2" in model_name:
                            model = Blip2ForConditionalGeneration.from_pretrained(
                                model_name,
                                torch_dtype=torch.float32,
                                device_map="auto"
                            )
                        else:
                            from transformers import AutoModelForVision2Seq
                            model = AutoModelForVision2Seq.from_pretrained(
                                model_name,
                                torch_dtype=torch.float32,
                                device_map="auto"
                            )
                        
                        model.eval()
                else:
                    # Re-raise if it's not an OOM error or FP16 error
                    logger.error(f"Unknown error during training: {e}")
                    logger.warning("Proceeding with model saving despite training errors")
            
            # Save model
            logger.info("Saving model")
            model.save_pretrained(output_dir)
            processor.save_pretrained(output_dir)
        except Exception as e:
            logger.error(f"Error during training: {e}")
            # Try to save the model anyway
            try:
                model.save_pretrained(output_dir)
                processor.save_pretrained(output_dir)
                logger.info("Saved model despite training error")
            except:
                logger.error("Could not save model after training error")
                raise
    
    # Generate a caption for the first training image to evaluate
    logger.info("Generating caption for the first training image to evaluate the model")
    
    # Load image
    image_path = train_dataset.dataset[0]["image_path"]
    image = Image.open(image_path).convert("RGB")
    
    try:
        # Process image only for BLIP models
        inputs = processor(images=image, return_tensors="pt").to(next(model.parameters()).device)
        
        # Generate with better settings
        with torch.no_grad():  # Disable gradient calculation for inference
            if "blip" in model.__class__.__name__.lower():
                # BLIP models can use text conditioning for more detailed captions
                # Define detailed prompts for BLIP models
                blip_prompts = [
                    "A detailed photo of",
                    "A detailed image showing",
                    "A high resolution photo that shows",
                    "A clear, detailed picture of",
                    "A photo with many details showing",
                    "A comprehensive image depicting",
                    "A detailed scene showing"
                ]
                
                # Select a prompt based on generation number
                prompt_idx = min(gen_number, len(blip_prompts) - 1)
                blip_prompt = blip_prompts[prompt_idx]
                
                try:
                    # Use conditional image captioning with the prompt
                    if hasattr(model, "generate_from_prefix"):
                        # Some BLIP models have this method
                        inputs = processor(images=image, return_tensors="pt").to(model.device)
                        with torch.no_grad():
                            generated_ids = model.generate_from_prefix(
                                pixel_values=inputs.pixel_values,
                                prefix=blip_prompt,
                                max_length=150,  # Increased from 80 for much longer captions
                                num_beams=5,
                                min_length=30,  # Increased from 20 to ensure longer, more detailed captions
                                do_sample=True,
                                temperature=0.7,
                                top_p=0.9,
                                top_k=50,
                                repetition_penalty=1.2,
                                no_repeat_ngram_size=2 if gen_number > 0 else None,
                                length_penalty=1.5  # Encourage longer captions
                            )
                    else:
                        # Standard approach with text conditioning
                        inputs = processor(images=image, text=blip_prompt, return_tensors="pt").to(model.device)
                        with torch.no_grad():
                            generated_ids = model.generate(
                                **inputs,
                                max_length=150,  # Increased from 80 for much longer captions
                                num_beams=5,
                                min_length=30,  # Increased from 20 to ensure longer, more detailed captions
                                do_sample=True,
                                temperature=0.7,
                                top_p=0.9,
                                top_k=50,
                                repetition_penalty=1.2,
                                no_repeat_ngram_size=2 if gen_number > 0 else None,
                                length_penalty=1.5  # Encourage longer captions
                            )
                    
                    # Decode the generated caption
                    if hasattr(processor, "batch_decode"):
                        caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                    else:
                        caption = processor.decode(generated_ids[0], skip_special_tokens=True)
                        
                    # Remove the prompt prefix if it's included
                    if caption.startswith(blip_prompt):
                        caption = caption[len(blip_prompt):].strip()
                except Exception as e:
                    logger.warning(f"Error in BLIP generation with prompt: {e}")
                    # Fall back to standard captioning
                    try:
                        inputs = processor(images=image, return_tensors="pt").to(model.device)
                        with torch.no_grad():
                            generated_ids = model.generate(
                                pixel_values=inputs.pixel_values,
                                max_length=150,  # Increased from 80 for much longer captions
                                num_beams=5,
                                min_length=30,  # Increased from 20 to ensure longer, more detailed captions
                                do_sample=True,
                                temperature=0.7,
                                top_p=0.9,
                                top_k=50,
                                repetition_penalty=1.2,
                                no_repeat_ngram_size=2 if gen_number > 0 else None,
                                length_penalty=1.5
                            )
                        
                        if hasattr(processor, "batch_decode"):
                            caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                        else:
                            caption = processor.decode(generated_ids[0], skip_special_tokens=True)
                    except Exception as e2:
                        logger.warning(f"Fallback generation also failed: {e2}")
                        caption = f"Error: {str(e2)}"
            else:
                # For other models, use the standard approach
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=150,  # Increased from 100 for much longer captions
                    num_beams=5,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.2
                )
                
                # Decode the generated caption
                if hasattr(processor, "batch_decode"):
                    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                else:
                    caption = processor.decode(generated_ids[0], skip_special_tokens=True)
            
            # Set the generated_caption variable
            generated_caption = caption
    except Exception as e:
        logger.error(f"Error during caption generation: {e}")
        generated_caption = f"Error generating caption: {str(e)}"
    
    logger.info(f"Original caption: {train_dataset.dataset[0]['caption']}")
    logger.info(f"Generated caption: {generated_caption}")
    
    # Save the evaluation result
    with open(os.path.join(output_dir, "evaluation.txt"), "w") as f:
        f.write(f"Training image: {image_path}\n")
        f.write(f"Original caption: {train_dataset.dataset[0]['caption']}\n")
        f.write(f"Generated caption: {generated_caption}\n")
    
    logger.info(f"Model saved to {output_dir}")
    return output_dir

def generate_captions(model_path, gen_number, image_prompt_pairs, output_dir=None):
    """Generate captions using the fine-tuned VLM."""
    if output_dir is None:
        output_dir = f"data/vlm/train/captions/gen_{gen_number}"
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Load model and processor
    processor = AutoProcessor.from_pretrained(model_path)
    
    # Load the correct model class based on the model configuration
    try:
        # Check if config.json exists to determine model type
        with open(os.path.join(model_path, "config.json"), "r") as f:
            config = json.load(f)
        
        model_type = config.get("model_type", "").lower()
        
        if "blip" in model_type and "blip2" not in model_type:
            from transformers import BlipForConditionalGeneration
            logger.info(f"Loading BLIP model from {model_path}")
            model = BlipForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                device_map="auto"
            )
        elif "blip2" in model_type:
            logger.info(f"Loading BLIP2 model from {model_path}")
            model = Blip2ForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                device_map="auto"
            )
        else:
            # For other models, try to determine the right class
            from transformers import AutoModelForVision2Seq
            logger.info(f"Loading Vision2Seq model from {model_path}")
            model = AutoModelForVision2Seq.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                device_map="auto"
            )
    except Exception as e:
        logger.warning(f"Error determining model type: {e}. Trying BlipForConditionalGeneration as fallback.")
        from transformers import BlipForConditionalGeneration
        model = BlipForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            device_map="auto"
        )
    
    # Set model to evaluation mode
    model.eval()
    
    # Set different generation parameters based on generation number to ensure diversity
    # Parameters tuned specifically for BLIP model based on empirical testing
    if gen_number == 0:
        # For generation 0, use standard parameters
        temperature = 0.8
        top_p = 0.9
        top_k = 50
        repetition_penalty = 1.2
        num_beams = 5
    else:
        # For later generations, increase diversity parameters
        # These values are carefully chosen to produce meaningful but diverse captions
        temperature = min(0.8 + (gen_number * 0.15), 1.5)  # Gradually increase temperature
        top_p = min(0.9 + (gen_number * 0.02), 0.98)  # Gradually increase top_p
        top_k = max(50 - (gen_number * 10), 20)  # Gradually decrease top_k
        repetition_penalty = min(1.2 + (gen_number * 0.1), 1.8)  # Gradually increase repetition penalty
        num_beams = max(5 - gen_number, 1)  # Reduce beam search for more diversity
    
    logger.info(f"Generation {gen_number} parameters: temperature={temperature:.2f}, top_p={top_p:.2f}, top_k={top_k}, repetition_penalty={repetition_penalty:.2f}, num_beams={num_beams}")
    
    results = []
    
    for i, (image_path, _) in enumerate(tqdm(image_prompt_pairs, desc="Generating captions")):
        try:
            # Load image
            image = Image.open(image_path).convert("RGB")
            
            # For BLIP models, we need to use a different approach for captioning
            if "blip" in model.__class__.__name__.lower():
                # BLIP models can use text conditioning for more detailed captions
                # Define detailed prompts for BLIP models
                blip_prompts = [
                    "A detailed photo of",
                    "A detailed image showing",
                    "A high resolution photo that shows",
                    "A clear, detailed picture of",
                    "A photo with many details showing",
                    "A comprehensive image depicting",
                    "A detailed scene showing"
                ]
                
                # Select a prompt based on generation number
                prompt_idx = min(gen_number, len(blip_prompts) - 1)
                blip_prompt = blip_prompts[prompt_idx]
                
                try:
                    # Use conditional image captioning with the prompt
                    if hasattr(model, "generate_from_prefix"):
                        # Some BLIP models have this method
                        inputs = processor(images=image, return_tensors="pt").to(model.device)
                        with torch.no_grad():
                            generated_ids = model.generate_from_prefix(
                                pixel_values=inputs.pixel_values,
                                prefix=blip_prompt,
                                max_length=150,  # Increased from 80 for much longer captions
                                num_beams=num_beams,
                                min_length=30,  # Increased from 20 to ensure longer, more detailed captions
                                do_sample=True,
                                temperature=temperature,
                                top_p=top_p,
                                top_k=top_k,
                                repetition_penalty=repetition_penalty,
                                no_repeat_ngram_size=2 if gen_number > 0 else None,
                                length_penalty=1.5  # Encourage longer captions
                            )
                    else:
                        # Standard approach with text conditioning
                        inputs = processor(images=image, text=blip_prompt, return_tensors="pt").to(model.device)
                        with torch.no_grad():
                            generated_ids = model.generate(
                                **inputs,
                                max_length=150,  # Increased from 80 for much longer captions
                                num_beams=num_beams,
                                min_length=30,  # Increased from 20 to ensure longer, more detailed captions
                                do_sample=True,
                                temperature=temperature,
                                top_p=top_p,
                                top_k=top_k,
                                repetition_penalty=repetition_penalty,
                                no_repeat_ngram_size=2 if gen_number > 0 else None,
                                length_penalty=1.5  # Encourage longer captions
                            )
                    
                    # Decode the generated caption
                    if hasattr(processor, "batch_decode"):
                        caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                    else:
                        caption = processor.decode(generated_ids[0], skip_special_tokens=True)
                        
                    # Remove the prompt prefix if it's included
                    if caption.startswith(blip_prompt):
                        caption = caption[len(blip_prompt):].strip()
                except Exception as e:
                    logger.warning(f"Error in BLIP generation with prompt: {e}")
                    # Fall back to standard captioning
                    try:
                        inputs = processor(images=image, return_tensors="pt").to(model.device)
                        with torch.no_grad():
                            generated_ids = model.generate(
                                pixel_values=inputs.pixel_values,
                                max_length=150,  # Increased from 80 for much longer captions
                                num_beams=num_beams,
                                min_length=30,  # Increased from 20 to ensure longer, more detailed captions
                                do_sample=True,
                                temperature=temperature,
                                top_p=top_p,
                                top_k=top_k,
                                repetition_penalty=repetition_penalty,
                                no_repeat_ngram_size=2 if gen_number > 0 else None,
                                length_penalty=1.5
                            )
                        
                        if hasattr(processor, "batch_decode"):
                            caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                        else:
                            caption = processor.decode(generated_ids[0], skip_special_tokens=True)
                    except Exception as e2:
                        logger.warning(f"Fallback generation also failed: {e2}")
                        caption = f"Error: {str(e2)}"
            else:
                # For other models, use the standard approach with text prompt
                # Vary the prompt slightly based on generation to encourage diversity
                prompts = [
                    "Describe this image in detail. Include all visible objects, colors, and activities.",
                    "Describe this image in detail. What can you see in this image? Include all important elements.",
                    "Give a detailed description of this image. Focus on objects, people, colors, and activities visible in the image.",
                    "Describe this image in detail. Explain what is happening in this image with all relevant details.",
                    "Provide a detailed caption for this image. Include all important visual elements and their relationships.",
                    "Describe this image in detail. What objects, people, and actions do you see in this image?",
                    "Describe this scene in detail. Include all visible elements, their colors, positions, and relationships."
                ]
                prompt_idx = min(gen_number, len(prompts) - 1)
                prompt = prompts[prompt_idx]
                
                inputs = processor(
                    images=image,
                    text=prompt,
                    return_tensors="pt",
                    padding="max_length",
                    truncation=True
                ).to(model.device)
                
                # Generate caption with appropriate parameters
                with torch.no_grad():
                    output = model.generate(
                        **inputs,
                        max_new_tokens=150,  # Increased from 100 for much longer captions
                        num_beams=num_beams,
                        early_stopping=True,
                        do_sample=True,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=top_k,
                        repetition_penalty=repetition_penalty,
                        no_repeat_ngram_size=2 if gen_number > 0 else None,
                        length_penalty=1.5  # Encourage longer captions
                    )
                    
                    # Decode caption
                    if hasattr(processor, "batch_decode"):
                        caption = processor.batch_decode(output, skip_special_tokens=True)[0]
                    else:
                        caption = processor.decode(output[0], skip_special_tokens=True)
                    
                    # Remove the prompt from the caption if it's included
                    if caption.startswith(prompt):
                        caption = caption[len(prompt):].strip()
            
            # Save result
            result = {
                "image_path": image_path,
                "caption": caption
            }
            results.append(result)
            
            # Save to file
            with open(os.path.join(output_dir, f"caption_{i}.json"), "w") as f:
                json.dump(result, f, indent=2)
            
        except Exception as e:
            logger.warning(f"Error generating caption for {image_path}: {e}")
            # Add a placeholder result to maintain the count
            result = {
                "image_path": image_path,
                "caption": f"Error: {str(e)}"
            }
            results.append(result)
    
    # Save all results
    with open(os.path.join(output_dir, "captions.json"), "w") as f:
        json.dump(results, f, indent=2)
    
    logger.info(f"Generated {len(results)} captions")
    return results

def evaluate_model(gen_number):
    """Evaluate the fine-tuned VLM."""
    model_path = f"models/vlm/gen_{gen_number}"
    output_dir = f"data/vlm_evaluation/gen_{gen_number}"
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Load test images and ground truth captions
    test_dir = "data/coco/val2014"
    annotation_file = "data/coco/annotations/captions_val2014.json"
    
    image_id_to_captions = load_coco_captions(annotation_file)
    
    # Get test images
    test_images = [os.path.join(test_dir, f) for f in os.listdir(test_dir) 
                  if f.endswith(('.jpg', '.jpeg', '.png'))]
    test_images = random.sample(test_images, min(100, len(test_images)))
    
    # Create image-prompt pairs
    image_prompt_pairs = []
    for image_path in test_images:
        # Extract image_id from filename
        image_id = int(os.path.basename(image_path).split('_')[-1].split('.')[0])
        
        if image_id in image_id_to_captions:
            prompt = "Describe this image in detail. Include all visible objects, colors, and activities."
            image_prompt_pairs.append((image_path, prompt))
    
    # Generate captions
    results = generate_captions(model_path, gen_number, image_prompt_pairs, output_dir)
    
    # Calculate metrics
    bleu = evaluate.load("bleu")
    rouge = evaluate.load("rouge")
    meteor = evaluate.load("meteor")
    
    references = []
    predictions = []
    
    for result in results:
        image_path = result["image_path"]
        image_id = int(os.path.basename(image_path).split('_')[-1].split('.')[0])
        
        if image_id in image_id_to_captions:
            ground_truth = image_id_to_captions[image_id]
            generated = result["caption"]
            
            references.append(ground_truth)
            predictions.append(generated)
    
    # Calculate metrics
    bleu_score = bleu.compute(predictions=predictions, references=references)
    rouge_score = rouge.compute(predictions=predictions, references=references)
    meteor_score = meteor.compute(predictions=predictions, references=references)
    
    # Save metrics
    metrics = {
        "bleu": bleu_score,
        "rouge": rouge_score,
        "meteor": meteor_score
    }
    
    with open(os.path.join(output_dir, "metrics.json"), "w") as f:
        json.dump(metrics, f, indent=2)
    
    logger.info(f"Evaluation metrics: {metrics}")
    return metrics

def find_last_generation():
    """Find the last generation number."""
    model_dir = "models/vlm"
    if not os.path.exists(model_dir):
        return -1
    
    gen_dirs = [d for d in os.listdir(model_dir) if d.startswith("gen_")]
    if not gen_dirs:
        return -1
    
    gen_numbers = [int(d.split("_")[1]) for d in gen_dirs]
    return max(gen_numbers)

def run_generation_loop(target_generation=10, epochs=10, batch_size=4, training_mode="recursive"):
    """Run the VLM fine-tuning loop for multiple generations.
    
    Args:
        target_generation: Number of generations to train (default: 10)
        epochs: Number of epochs per generation
        batch_size: Batch size for training
        training_mode: One of ["recursive", "baseline", "gen0"]
            - recursive: Use captions from previous generation (default)
            - baseline: Use original COCO captions
            - gen0: Use captions generated by gen_0 model
    """
    current_gen = find_last_generation()
    
    if current_gen == -1:
        # Start from scratch with pretrained model (gen 0)
        logger.info("Starting from generation 0 (pretrained model)")
        current_gen = 0
        
        # For gen 0, we'll save the pretrained model without training
        logger.info("Setting up generation 0 (pretrained model without training)")
        output_model_path = train_vlm(
            gen_number=0,
            input_model_path=None,
            epochs=1,
            batch_size=1,
            skip_training=True,  # Skip training for gen 0
            training_mode=training_mode
        )
        
        # Now set up for gen 1
        input_model_path = output_model_path
        
        # Ensure captions are generated for gen_0
        gen0_captions_file = "data/vlm/train/captions/gen_0/captions.json"
        if not os.path.exists(gen0_captions_file):
            logger.info("Generating captions for generation 0")
            # Get all images from train/real
            train_dir = "data/vlm/train/images/real"
            image_paths = glob.glob(os.path.join(train_dir, "*.jpg"))
            if not image_paths:
                image_paths = glob.glob(os.path.join(train_dir, "*.png"))
            
            # Create image-prompt pairs
            image_prompt_pairs = [(img_path, "Give a detailed description of this image. Focus on objects, people, colors, and activities visible in the image.") for img_path in image_paths]
            
            # Generate captions for all images
            captions_dir = "data/vlm/train/captions/gen_0"
            generate_captions(output_model_path, 0, image_prompt_pairs, captions_dir)
    else:
        # Continue from last generation
        model_path = f"models/vlm/gen_{current_gen}_{training_mode}"
        # Check if the directory contains actual model files
        if os.path.exists(os.path.join(model_path, "config.json")):
            logger.info(f"Continuing from generation {current_gen}")
            input_model_path = model_path
            
            # Ensure captions exist for the current generation
            current_captions_file = f"data/vlm/train/captions/gen_{current_gen}/captions.json"
            if not os.path.exists(current_captions_file):
                logger.info(f"Generating captions for generation {current_gen}")
                # Get all images from train/real
                train_dir = "data/vlm/train/images/real"
                image_paths = glob.glob(os.path.join(train_dir, "*.jpg"))
                if not image_paths:
                    image_paths = glob.glob(os.path.join(train_dir, "*.png"))
                
                # Create image-prompt pairs
                image_prompt_pairs = [(img_path, "Give a detailed description of this image. Focus on objects, people, colors, and activities visible in the image.") for img_path in image_paths]
                
                # Generate captions for all images
                captions_dir = f"data/vlm/train/captions/gen_{current_gen}"
                generate_captions(input_model_path, current_gen, image_prompt_pairs, captions_dir)
        else:
            logger.info(f"Generation {current_gen} directory exists but doesn't contain a valid model. Starting from scratch.")
            current_gen = 0
            
            # For gen 0, we'll save the pretrained model without training
            logger.info("Setting up generation 0 (pretrained model without training)")
            output_model_path = train_vlm(
                gen_number=0,
                input_model_path=None,
                epochs=1,
                batch_size=1,
                skip_training=True,  # Skip training for gen 0
                training_mode=training_mode
            )
            
            # Now set up for gen 1
            input_model_path = output_model_path
            
            # Ensure captions are generated for gen_0
            gen0_captions_file = "data/vlm/train/captions/gen_0/captions.json"
            if not os.path.exists(gen0_captions_file):
                logger.info("Generating captions for generation 0")
                # Get all images from train/real
                train_dir = "data/vlm/train/images/real"
                image_paths = glob.glob(os.path.join(train_dir, "*.jpg"))
                if not image_paths:
                    image_paths = glob.glob(os.path.join(train_dir, "*.png"))
                
                # Create image-prompt pairs
                image_prompt_pairs = [(img_path, "Give a detailed description of this image. Focus on objects, people, colors, and activities visible in the image.") for img_path in image_paths]
                
                # Generate captions for all images
                captions_dir = "data/vlm/train/captions/gen_0"
                generate_captions(output_model_path, 0, image_prompt_pairs, captions_dir)
    
    while current_gen < target_generation:
        next_gen = current_gen + 1
        logger.info(f"Training generation {next_gen}")
        
        # For gen 1, we use the gen 0 model
        # For gen 2+, we use the previous generation's model
        
        # Ensure captions exist for the current generation before training the next one
        current_captions_file = f"data/vlm/train/captions/gen_{current_gen}/captions.json"
        if not os.path.exists(current_captions_file):
            logger.info(f"Generating captions for generation {current_gen}")
            # Get all images from train/real
            train_dir = "data/vlm/train/images/real"
            image_paths = glob.glob(os.path.join(train_dir, "*.jpg"))
            if not image_paths:
                image_paths = glob.glob(os.path.join(train_dir, "*.png"))
            
            # Create image-prompt pairs
            image_prompt_pairs = [(img_path, "Give a detailed description of this image. Focus on objects, people, colors, and activities visible in the image.") for img_path in image_paths]
            
            # Generate captions for all images
            captions_dir = f"data/vlm/train/captions/gen_{current_gen}"
            generate_captions(input_model_path, current_gen, image_prompt_pairs, captions_dir)
        
        # Train model for the next generation using captions from the current generation
        output_model_path = train_vlm(
            gen_number=next_gen,
            input_model_path=input_model_path,
            epochs=epochs,
            batch_size=batch_size,
            skip_training=False,  # Do training for gen 1+
            training_mode=training_mode
        )
        
        # Update for next iteration
        current_gen = next_gen
        input_model_path = output_model_path
    
    return current_gen

def main():
    parser = argparse.ArgumentParser(description="Train a VLM for captioning")
    parser.add_argument("--generations", type=int, default=10, help="Number of generations to train")
    parser.add_argument("--epochs", type=int, default=10, help="Number of epochs per generation")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for training")
    parser.add_argument("--training_mode", type=str, default="recursive", choices=["recursive", "baseline", "gen0"], 
                      help="Training mode: recursive (use previous gen captions), baseline (use original COCO captions), or gen0 (use gen_0 captions)")
    
    args = parser.parse_args()
    
    # Set random seed for reproducibility
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    
    # Run the generation loop
    final_gen = run_generation_loop(
        target_generation=args.generations,
        epochs=args.epochs,
        batch_size=args.batch_size,
        training_mode=args.training_mode
    )
    
    logger.info(f"Training completed through generation {final_gen}")
    logger.info(f"Final model saved at: models/vlm/gen_{final_gen}_{args.training_mode}")

if __name__ == "__main__":
    main() 